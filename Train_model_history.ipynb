{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64,64,3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 250, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15560 images belonging to 250 classes.\n",
      "Found 5000 images belonging to 250 classes.\n",
      "Epoch 1/10\n",
      "15560/15560 [==============================] - 1915s 123ms/step - loss: 4.0823 - acc: 0.1697 - val_loss: 3.6356 - val_acc: 0.2496\n",
      "Epoch 2/10\n",
      "15560/15560 [==============================] - 1873s 120ms/step - loss: 2.4327 - acc: 0.4094 - val_loss: 3.8557 - val_acc: 0.2874\n",
      "Epoch 3/10\n",
      "15560/15560 [==============================] - 1876s 121ms/step - loss: 2.0281 - acc: 0.4866 - val_loss: 3.9672 - val_acc: 0.2857\n",
      "Epoch 4/10\n",
      "15560/15560 [==============================] - 1887s 121ms/step - loss: 1.8065 - acc: 0.5307 - val_loss: 4.2254 - val_acc: 0.2892\n",
      "Epoch 5/10\n",
      "15560/15560 [==============================] - 1888s 121ms/step - loss: 1.6442 - acc: 0.5656 - val_loss: 4.6150 - val_acc: 0.2822\n",
      "Epoch 6/10\n",
      "15560/15560 [==============================] - 1886s 121ms/step - loss: 1.5180 - acc: 0.5922 - val_loss: 4.8374 - val_acc: 0.2930\n",
      "Epoch 7/10\n",
      "15560/15560 [==============================] - 1875s 120ms/step - loss: 1.3853 - acc: 0.6217 - val_loss: 4.8671 - val_acc: 0.2915\n",
      "Epoch 8/10\n",
      "15560/15560 [==============================] - 1879s 121ms/step - loss: 1.2633 - acc: 0.6506 - val_loss: 5.3360 - val_acc: 0.2870\n",
      "Epoch 9/10\n",
      "15560/15560 [==============================] - 1882s 121ms/step - loss: 1.1617 - acc: 0.6753 - val_loss: 5.4620 - val_acc: 0.2873\n",
      "Epoch 10/10\n",
      "15560/15560 [==============================] - 1904s 122ms/step - loss: 1.0795 - acc: 0.6954 - val_loss: 5.6158 - val_acc: 0.2999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25f5a69be0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/kazi/MR/train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/home/kazi/MR/test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 15560,\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# save/ load syntax\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "classifier.save('sketchmodel.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "classifier.save('sketchmodel.hdf5')\n",
    "#del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parking meter\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/home/kazi/random5.jpg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis =0)\n",
    "result = classifier.predict(test_image)\n",
    "#training_set.class_indices\n",
    "\n",
    "idxarr = result.argmax(axis=1)\n",
    "index = idxarr[0]\n",
    "prediction = []\n",
    "for key, indexx in training_set.class_indices.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if indexx == index:\n",
    "        prediction = key\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'alarm clock': 1,\n",
       " 'angel': 2,\n",
       " 'ant': 3,\n",
       " 'apple': 4,\n",
       " 'arm': 5,\n",
       " 'armchair': 6,\n",
       " 'ashtray': 7,\n",
       " 'axe': 8,\n",
       " 'backpack': 9,\n",
       " 'banana': 10,\n",
       " 'barn': 11,\n",
       " 'baseball bat': 12,\n",
       " 'basket': 13,\n",
       " 'bathtub': 14,\n",
       " 'bear (animal)': 15,\n",
       " 'bed': 16,\n",
       " 'bee': 17,\n",
       " 'beer-mug': 18,\n",
       " 'bell': 19,\n",
       " 'bench': 20,\n",
       " 'bicycle': 21,\n",
       " 'binoculars': 22,\n",
       " 'blimp': 23,\n",
       " 'book': 24,\n",
       " 'bookshelf': 25,\n",
       " 'boomerang': 26,\n",
       " 'bottle opener': 27,\n",
       " 'bowl': 28,\n",
       " 'brain': 29,\n",
       " 'bread': 30,\n",
       " 'bridge': 31,\n",
       " 'bulldozer': 32,\n",
       " 'bus': 33,\n",
       " 'bush': 34,\n",
       " 'butterfly': 35,\n",
       " 'cabinet': 36,\n",
       " 'cactus': 37,\n",
       " 'cake': 38,\n",
       " 'calculator': 39,\n",
       " 'camel': 40,\n",
       " 'camera': 41,\n",
       " 'candle': 42,\n",
       " 'cannon': 43,\n",
       " 'canoe': 44,\n",
       " 'car (sedan)': 45,\n",
       " 'carrot': 46,\n",
       " 'castle': 47,\n",
       " 'cat': 48,\n",
       " 'cell phone': 49,\n",
       " 'chair': 50,\n",
       " 'chandelier': 51,\n",
       " 'church': 52,\n",
       " 'cigarette': 53,\n",
       " 'cloud': 54,\n",
       " 'comb': 55,\n",
       " 'computer monitor': 56,\n",
       " 'computer-mouse': 57,\n",
       " 'couch': 58,\n",
       " 'cow': 59,\n",
       " 'crab': 60,\n",
       " 'crane (machine)': 61,\n",
       " 'crocodile': 62,\n",
       " 'crown': 63,\n",
       " 'cup': 64,\n",
       " 'diamond': 65,\n",
       " 'dog': 66,\n",
       " 'dolphin': 67,\n",
       " 'donut': 68,\n",
       " 'door': 69,\n",
       " 'door handle': 70,\n",
       " 'dragon': 71,\n",
       " 'duck': 72,\n",
       " 'ear': 73,\n",
       " 'elephant': 74,\n",
       " 'envelope': 75,\n",
       " 'eye': 76,\n",
       " 'eyeglasses': 77,\n",
       " 'face': 78,\n",
       " 'fan': 79,\n",
       " 'feather': 80,\n",
       " 'fire hydrant': 81,\n",
       " 'fish': 82,\n",
       " 'flashlight': 83,\n",
       " 'floor lamp': 84,\n",
       " 'flower with stem': 85,\n",
       " 'flying bird': 86,\n",
       " 'flying saucer': 87,\n",
       " 'foot': 88,\n",
       " 'fork': 89,\n",
       " 'frog': 90,\n",
       " 'frying-pan': 91,\n",
       " 'giraffe': 92,\n",
       " 'grapes': 93,\n",
       " 'grenade': 94,\n",
       " 'guitar': 95,\n",
       " 'hamburger': 96,\n",
       " 'hammer': 97,\n",
       " 'hand': 98,\n",
       " 'harp': 99,\n",
       " 'hat': 100,\n",
       " 'head': 101,\n",
       " 'head-phones': 102,\n",
       " 'hedgehog': 103,\n",
       " 'helicopter': 104,\n",
       " 'helmet': 105,\n",
       " 'horse': 106,\n",
       " 'hot air balloon': 107,\n",
       " 'hot-dog': 108,\n",
       " 'hourglass': 109,\n",
       " 'house': 110,\n",
       " 'human-skeleton': 111,\n",
       " 'ice-cream-cone': 112,\n",
       " 'ipod': 113,\n",
       " 'kangaroo': 114,\n",
       " 'key': 115,\n",
       " 'keyboard': 116,\n",
       " 'knife': 117,\n",
       " 'ladder': 118,\n",
       " 'laptop': 119,\n",
       " 'leaf': 120,\n",
       " 'lightbulb': 121,\n",
       " 'lighter': 122,\n",
       " 'lion': 123,\n",
       " 'lobster': 124,\n",
       " 'loudspeaker': 125,\n",
       " 'mailbox': 126,\n",
       " 'megaphone': 127,\n",
       " 'mermaid': 128,\n",
       " 'microphone': 129,\n",
       " 'microscope': 130,\n",
       " 'monkey': 131,\n",
       " 'moon': 132,\n",
       " 'mosquito': 133,\n",
       " 'motorbike': 134,\n",
       " 'mouse (animal)': 135,\n",
       " 'mouth': 136,\n",
       " 'mug': 137,\n",
       " 'mushroom': 138,\n",
       " 'nose': 139,\n",
       " 'octopus': 140,\n",
       " 'owl': 141,\n",
       " 'palm tree': 142,\n",
       " 'panda': 143,\n",
       " 'paper clip': 144,\n",
       " 'parachute': 145,\n",
       " 'parking meter': 146,\n",
       " 'parrot': 147,\n",
       " 'pear': 148,\n",
       " 'pen': 149,\n",
       " 'penguin': 150,\n",
       " 'person sitting': 151,\n",
       " 'person walking': 152,\n",
       " 'piano': 153,\n",
       " 'pickup truck': 154,\n",
       " 'pig': 155,\n",
       " 'pigeon': 156,\n",
       " 'pineapple': 157,\n",
       " 'pipe (for smoking)': 158,\n",
       " 'pizza': 159,\n",
       " 'potted plant': 160,\n",
       " 'power outlet': 161,\n",
       " 'present': 162,\n",
       " 'pretzel': 163,\n",
       " 'pumpkin': 164,\n",
       " 'purse': 165,\n",
       " 'rabbit': 166,\n",
       " 'race car': 167,\n",
       " 'radio': 168,\n",
       " 'rainbow': 169,\n",
       " 'revolver': 170,\n",
       " 'rifle': 171,\n",
       " 'rollerblades': 172,\n",
       " 'rooster': 173,\n",
       " 'sailboat': 174,\n",
       " 'santa claus': 175,\n",
       " 'satellite': 176,\n",
       " 'satellite dish': 177,\n",
       " 'saxophone': 178,\n",
       " 'scissors': 179,\n",
       " 'scorpion': 180,\n",
       " 'screwdriver': 181,\n",
       " 'sea turtle': 182,\n",
       " 'seagull': 183,\n",
       " 'shark': 184,\n",
       " 'sheep': 185,\n",
       " 'ship': 186,\n",
       " 'shoe': 187,\n",
       " 'shovel': 188,\n",
       " 'skateboard': 189,\n",
       " 'skull': 190,\n",
       " 'skyscraper': 191,\n",
       " 'snail': 192,\n",
       " 'snake': 193,\n",
       " 'snowboard': 194,\n",
       " 'snowman': 195,\n",
       " 'socks': 196,\n",
       " 'space shuttle': 197,\n",
       " 'speed-boat': 198,\n",
       " 'spider': 199,\n",
       " 'sponge bob': 200,\n",
       " 'spoon': 201,\n",
       " 'squirrel': 202,\n",
       " 'standing bird': 203,\n",
       " 'stapler': 204,\n",
       " 'strawberry': 205,\n",
       " 'streetlight': 206,\n",
       " 'submarine': 207,\n",
       " 'suitcase': 208,\n",
       " 'sun': 209,\n",
       " 'suv': 210,\n",
       " 'swan': 211,\n",
       " 'sword': 212,\n",
       " 'syringe': 213,\n",
       " 't-shirt': 214,\n",
       " 'table': 215,\n",
       " 'tablelamp': 216,\n",
       " 'teacup': 217,\n",
       " 'teapot': 218,\n",
       " 'teddy-bear': 219,\n",
       " 'telephone': 220,\n",
       " 'tennis-racket': 221,\n",
       " 'tent': 222,\n",
       " 'tiger': 223,\n",
       " 'tire': 224,\n",
       " 'toilet': 225,\n",
       " 'tomato': 226,\n",
       " 'tooth': 227,\n",
       " 'toothbrush': 228,\n",
       " 'tractor': 229,\n",
       " 'traffic light': 230,\n",
       " 'train': 231,\n",
       " 'tree': 232,\n",
       " 'trombone': 233,\n",
       " 'trousers': 234,\n",
       " 'truck': 235,\n",
       " 'trumpet': 236,\n",
       " 'tv': 237,\n",
       " 'umbrella': 238,\n",
       " 'van': 239,\n",
       " 'vase': 240,\n",
       " 'violin': 241,\n",
       " 'walkie talkie': 242,\n",
       " 'wheel': 243,\n",
       " 'wheelbarrow': 244,\n",
       " 'windmill': 245,\n",
       " 'wine-bottle': 246,\n",
       " 'wineglass': 247,\n",
       " 'wrist-watch': 248,\n",
       " 'zebra': 249}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### try 2 with 56 classes\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64,64,3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 56, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3920 images belonging to 56 classes.\n",
      "Found 1120 images belonging to 56 classes.\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 1099s 110ms/step - loss: 1.5711 - acc: 0.5677 - val_loss: 2.3618 - val_acc: 0.6314\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 1033s 103ms/step - loss: 0.4646 - acc: 0.8571 - val_loss: 2.9389 - val_acc: 0.6671\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 1007s 101ms/step - loss: 0.2614 - acc: 0.9182 - val_loss: 3.1612 - val_acc: 0.6733\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 1022s 102ms/step - loss: 0.1912 - acc: 0.9403 - val_loss: 3.4430 - val_acc: 0.6666\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 1027s 103ms/step - loss: 0.1537 - acc: 0.9522 - val_loss: 3.5300 - val_acc: 0.6662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25f43f76a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/kazi/MR/train2',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/home/kazi/MR/test2',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 10000,\n",
    "                         epochs = 5,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('sketchmodel56.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "classifier.save('sketchmodel56.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannon\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/home/kazi/random4.jpg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis =0)\n",
    "result = classifier.predict(test_image)\n",
    "#training_set.class_indices\n",
    "\n",
    "idxarr = result.argmax(axis=1)\n",
    "index = idxarr[0]\n",
    "prediction = []\n",
    "for key, indexx in training_set.class_indices.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if indexx == index:\n",
    "        prediction = key\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'alarm clock': 1,\n",
       " 'angel': 2,\n",
       " 'ant': 3,\n",
       " 'apple': 4,\n",
       " 'arm': 5,\n",
       " 'armchair': 6,\n",
       " 'ashtray': 7,\n",
       " 'axe': 8,\n",
       " 'backpack': 9,\n",
       " 'banana': 10,\n",
       " 'barn': 11,\n",
       " 'baseball bat': 12,\n",
       " 'basket': 13,\n",
       " 'bathtub': 14,\n",
       " 'bear (animal)': 15,\n",
       " 'bed': 16,\n",
       " 'bee': 17,\n",
       " 'beer-mug': 18,\n",
       " 'bell': 19,\n",
       " 'bench': 20,\n",
       " 'bicycle': 21,\n",
       " 'binoculars': 22,\n",
       " 'blimp': 23,\n",
       " 'book': 24,\n",
       " 'bookshelf': 25,\n",
       " 'boomerang': 26,\n",
       " 'bottle opener': 27,\n",
       " 'bowl': 28,\n",
       " 'brain': 29,\n",
       " 'bread': 30,\n",
       " 'bridge': 31,\n",
       " 'bulldozer': 32,\n",
       " 'bus': 33,\n",
       " 'bush': 34,\n",
       " 'butterfly': 35,\n",
       " 'cabinet': 36,\n",
       " 'cactus': 37,\n",
       " 'cake': 38,\n",
       " 'calculator': 39,\n",
       " 'camel': 40,\n",
       " 'camera': 41,\n",
       " 'candle': 42,\n",
       " 'cannon': 43,\n",
       " 'canoe': 44,\n",
       " 'car (sedan)': 45,\n",
       " 'carrot': 46,\n",
       " 'castle': 47,\n",
       " 'cat': 48,\n",
       " 'cell phone': 49,\n",
       " 'chair': 50,\n",
       " 'chandelier': 51,\n",
       " 'church': 52,\n",
       " 'cigarette': 53,\n",
       " 'cloud': 54,\n",
       " 'comb': 55}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### try 2 with 30 classes\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64,64,3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 30, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2360 images belonging to 30 classes.\n",
      "Found 600 images belonging to 30 classes.\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 815s 102ms/step - loss: 0.6400 - acc: 0.8097 - val_loss: 0.3167 - val_acc: 0.9547\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 811s 101ms/step - loss: 0.0880 - acc: 0.9723 - val_loss: 0.2694 - val_acc: 0.9701\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 807s 101ms/step - loss: 0.0553 - acc: 0.9826 - val_loss: 0.2257 - val_acc: 0.9782\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 808s 101ms/step - loss: 0.0421 - acc: 0.9870 - val_loss: 0.3026 - val_acc: 0.9716\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 811s 101ms/step - loss: 0.0354 - acc: 0.9893 - val_loss: 0.2572 - val_acc: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25f4b3d908>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/kazi/MR/train3',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/home/kazi/MR/test3',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 5,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('sketchmodel30.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "classifier.save('sketchmodel30.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/home/kazi/random3.jpg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis =0)\n",
    "result = classifier.predict(test_image)\n",
    "#training_set.class_indices\n",
    "\n",
    "idxarr = result.argmax(axis=1)\n",
    "index = idxarr[0]\n",
    "prediction = []\n",
    "for key, indexx in training_set.class_indices.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if indexx == index:\n",
    "        prediction = key\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "####################### trying with 30b\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64,64,3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 21, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1260 images belonging to 21 classes.\n",
      "Found 420 images belonging to 21 classes.\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 512s 102ms/step - loss: 0.9067 - acc: 0.7178 - val_loss: 4.5327 - val_acc: 0.4355\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 512s 102ms/step - loss: 0.0970 - acc: 0.9690 - val_loss: 5.3965 - val_acc: 0.4216\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 511s 102ms/step - loss: 0.0543 - acc: 0.9828 - val_loss: 5.6580 - val_acc: 0.4092\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 508s 102ms/step - loss: 0.0420 - acc: 0.9868 - val_loss: 5.8652 - val_acc: 0.4357\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 532s 106ms/step - loss: 0.0324 - acc: 0.9896 - val_loss: 6.4609 - val_acc: 0.4334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b42af70f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/kazi/MR/train4',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/home/kazi/MR/test4',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 5000,\n",
    "                         epochs = 5,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('sketchmodel20.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "classifier.save('sketchmodel20.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#with sketchy data\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (256,256,3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 125, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75481 images belonging to 125 classes.\n",
      "Found 11189 images belonging to 125 classes.\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - -5740s -1148011us/step - loss: 3.5904 - acc: 0.2010 - val_loss: 3.6066 - val_acc: 0.2081\n",
      "Epoch 2/10\n",
      "2988/5000 [================>.............] - ETA: 9:31 - loss: 2.7143 - acc: 0.3483"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1535s 307ms/step - loss: 1.8568 - acc: 0.5239 - val_loss: 5.1870 - val_acc: 0.2181\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 1490s 298ms/step - loss: 1.7695 - acc: 0.5450 - val_loss: 4.5819 - val_acc: 0.2193\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 1639s 328ms/step - loss: 1.7151 - acc: 0.5570 - val_loss: 6.5098 - val_acc: 0.1884\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 1509s 302ms/step - loss: 1.6387 - acc: 0.5727 - val_loss: 4.4737 - val_acc: 0.2377\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 1475s 295ms/step - loss: 1.5955 - acc: 0.5824 - val_loss: 4.4954 - val_acc: 0.2382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbefbebad68>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/kazi/MR/sketchyTrain',\n",
    "                                                 target_size = (256, 256),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/home/kazi/MR/sketchyTest',\n",
    "                                            target_size = (256, 256),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 5000,\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('sketchmodel125.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "classifier.save('sketchmodel125.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\injam\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (15, 15), activation=\"relu\", strides=(3, 3))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 256, 256)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 81, 81)   43264       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 40, 40)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 36, 36)  204928      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 128, 17, 17)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 13, 13)  819456      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 256, 6, 6)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 48, 6, 6)     12336       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 256, 4, 4)    0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 4, 4)     27712       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 4, 4)     16448       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 4, 4)     147520      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192, 4, 4)    0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 192, 1, 1)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 192)          0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          49408       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 125)          32125       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,353,197\n",
      "Trainable params: 1,353,197\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\injam\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), activation=\"relu\", strides=(1, 1))`\n",
      "C:\\Users\\injam\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (5, 5), activation=\"relu\", strides=(1, 1))`\n",
      "C:\\Users\\injam\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (1, 1))`\n",
      "C:\\Users\\injam\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:60: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "C:\\Users\\injam\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1))`\n",
      "C:\\Users\\injam\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    }
   ],
   "source": [
    "############with different network\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "train_data_dir = 'sketchyTrain/'\n",
    "validation_data_dir = 'sketchyTest/'\n",
    "nb_train_samples = 75481\n",
    "nb_validation_samples = 11189\n",
    "nb_epoch = 10\n",
    "\n",
    "# checkpoint\n",
    "\"\"\"\n",
    "    save model weights when validation accuracy increases during training\n",
    "\"\"\"\n",
    "filepath=\"bestWeight125.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    merge,\n",
    "    Lambda\n",
    ")\n",
    "from keras.layers.convolutional import (\n",
    "    Convolution2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D,\n",
    "    ZeroPadding2D\n",
    ")\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "input = Input(shape=(3, 256, 256))\n",
    "\n",
    "layer1_conv_1 = Convolution2D(64, 15, 15,subsample=(3, 3),activation='relu')(input)\n",
    "layer1_pool_1 = MaxPooling2D(pool_size=(3, 3),strides=(2, 2))(layer1_conv_1)\n",
    "\n",
    "layer2_conv_1 = Convolution2D(128, 5, 5,subsample=(1, 1),activation='relu')(layer1_pool_1)\n",
    "layer2_pool_1 = MaxPooling2D(pool_size=(3, 3),strides=(2, 2))(layer2_conv_1)\n",
    "\n",
    "layer3_conv_1 = Convolution2D(256, 5, 5,subsample=(1, 1),activation='relu')(layer2_pool_1)\n",
    "layer3_pool_1 = MaxPooling2D(pool_size=(3, 3),strides=(2, 2))(layer3_conv_1)\n",
    "\n",
    "#tower A\n",
    "sparse_conv_a1 = Convolution2D(48, 1, 1)(layer3_pool_1)\n",
    "sparse_conv_a2 = Convolution2D(64, 3, 3)(sparse_conv_a1)\n",
    "\n",
    "\n",
    "# Tower B\n",
    "sparse_pool_b1 = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(layer3_pool_1)\n",
    "sparse_conv_b2 = Convolution2D(64, 1, 1)(sparse_pool_b1)\n",
    "\n",
    "# Tower C\n",
    "sparse_conv_c1 = Convolution2D(64, 3, 3)(layer3_pool_1)\n",
    "\n",
    "#merge_layer = merge([sparse_conv_a2, sparse_conv_b2, sparse_conv_c1], mode='concat', concat_axis=1)\n",
    "merge_layer = concatenate([sparse_conv_a2, sparse_conv_b2, sparse_conv_c1], axis=1)\n",
    "\n",
    "layer5_pool_1 = MaxPooling2D(pool_size=(3, 3),strides=(2, 2))(merge_layer)\n",
    "flat = Flatten()(layer5_pool_1)\n",
    "fc1 = Dense(256, activation='relu')(flat)\n",
    "dr = Dropout(0.5)(fc1)\n",
    "fc2 = Dense(125, activation='sigmoid')(dr)\n",
    "model = Model(input,fc2)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "# print model summary \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75481 images belonging to 125 classes.\n",
      "Found 11189 images belonging to 125 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                 target_size = (256, 256),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "                                            target_size = (256, 256),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 6855s 685ms/step - loss: 4.8255 - acc: 0.0094 - val_loss: 4.8328 - val_acc: 0.0072\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 6313s 631ms/step - loss: 4.8238 - acc: 0.0095 - val_loss: 4.8364 - val_acc: 0.0072\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 6373s 637ms/step - loss: 4.8238 - acc: 0.0097 - val_loss: 4.8364 - val_acc: 0.0071\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 6777s 678ms/step - loss: 4.8237 - acc: 0.0096 - val_loss: 4.8364 - val_acc: 0.0071\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 6552s 655ms/step - loss: 4.8238 - acc: 0.0095 - val_loss: 4.8364 - val_acc: 0.0071\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 6324s 632ms/step - loss: 4.8237 - acc: 0.0095 - val_loss: 4.8370 - val_acc: 0.0071\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 6308s 631ms/step - loss: 4.8238 - acc: 0.0094 - val_loss: 4.8361 - val_acc: 0.0071\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 6315s 632ms/step - loss: 4.8237 - acc: 0.0095 - val_loss: 4.8363 - val_acc: 0.0071\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 6317s 632ms/step - loss: 4.8238 - acc: 0.0095 - val_loss: 4.8364 - val_acc: 0.0088\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 6399s 640ms/step - loss: 4.8237 - acc: 0.0095 - val_loss: 4.8367 - val_acc: 0.0071\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 6307s 631ms/step - loss: 4.8237 - acc: 0.0096 - val_loss: 4.8363 - val_acc: 0.0072\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 6319s 632ms/step - loss: 4.8238 - acc: 0.0095 - val_loss: 4.8365 - val_acc: 0.0071\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 6319s 632ms/step - loss: 4.8238 - acc: 0.0095 - val_loss: 4.8362 - val_acc: 0.0071\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 6536s 654ms/step - loss: 4.8237 - acc: 0.0095 - val_loss: 4.8361 - val_acc: 0.0088\n",
      "Epoch 15/20\n",
      " 1084/10000 [==>...........................] - ETA: 1:24:59 - loss: 4.8242 - acc: 0.0089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b4a069e2a964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                          validation_steps = 5000)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow36\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## training function \n",
    "history = model.fit_generator(train_generator,\n",
    "                         steps_per_epoch = 10000,\n",
    "                         epochs = 20,\n",
    "                         validation_data = test_generator,\n",
    "                         validation_steps = 5000)\n",
    "\n",
    "\n",
    "model.save('newTrain125.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "model.save('newTrain125.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1700857543031301961\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3279241216\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13978647931342946416\n",
      "physical_device_desc: \"device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model125.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75481 images belonging to 125 classes.\n",
      "Found 11189 images belonging to 125 classes.\n",
      "Epoch 1/10\n",
      "4718/4717 [==============================] - 4613s - loss: 3.2348 - acc: 0.2565 - val_loss: 3.0426 - val_acc: 0.2789\n",
      "Epoch 2/10\n",
      "4718/4717 [==============================] - 4555s - loss: 2.3135 - acc: 0.4225 - val_loss: 3.2195 - val_acc: 0.2794\n",
      "Epoch 3/10\n",
      "4718/4717 [==============================] - 4550s - loss: 2.0228 - acc: 0.4822 - val_loss: 3.3857 - val_acc: 0.2674\n",
      "Epoch 4/10\n",
      "4718/4717 [==============================] - 4604s - loss: 1.8479 - acc: 0.5208 - val_loss: 3.5725 - val_acc: 0.2503\n",
      "Epoch 5/10\n",
      "4718/4717 [==============================] - 4611s - loss: 1.7298 - acc: 0.5456 - val_loss: 4.1603 - val_acc: 0.2485\n",
      "Epoch 6/10\n",
      "4718/4717 [==============================] - 4641s - loss: 1.6387 - acc: 0.5666 - val_loss: 4.0522 - val_acc: 0.2527\n",
      "Epoch 7/10\n",
      "4718/4717 [==============================] - 4635s - loss: 1.5750 - acc: 0.5804 - val_loss: 4.5173 - val_acc: 0.2567\n",
      "Epoch 8/10\n",
      "4718/4717 [==============================] - 4636s - loss: 1.5130 - acc: 0.5961 - val_loss: 3.5632 - val_acc: 0.2865\n",
      "Epoch 9/10\n",
      "4718/4717 [==============================] - 4684s - loss: 1.4648 - acc: 0.6054 - val_loss: 4.3280 - val_acc: 0.2693\n",
      "Epoch 10/10\n",
      "4718/4717 [==============================] - 4589s - loss: 1.4264 - acc: 0.6139 - val_loss: 3.9871 - val_acc: 0.2638\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "n_train = 75481\n",
    "n_test = 11189\n",
    "batch_size = 32\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (256,256,3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(16, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a third convolutional layer\n",
    "classifier.add(Conv2D(8, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 256, activation = 'relu'))\n",
    "classifier.add(Dense(units = 125, activation = 'softmax'))\n",
    "# classifier.add(Activation(tf.nn.softmax))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('sketchyTrain/',\n",
    "                                                 target_size = (256, 256),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('sketchyTest/',\n",
    "                                            target_size = (256, 256),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(classifier, to_file='modelfinal125.png')\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 2*(n_train/batch_size),\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "classifier.save('sketchmodel125final.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "classifier.save('sketchmodel125final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helicopter\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('testing/image9.png', target_size = (256,256))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis =0)\n",
    "result = classifier.predict(test_image)\n",
    "#training_set.class_indices\n",
    "\n",
    "idxarr = result.argmax(axis=1)\n",
    "index = idxarr[0]\n",
    "prediction = []\n",
    "for key, indexx in training_set.class_indices.items():    # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "    if indexx == index:\n",
    "        prediction = key\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
